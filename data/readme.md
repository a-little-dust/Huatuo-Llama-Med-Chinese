## infer文件
提供了几组 instruction 和 output，作为示例数据，帮助验证模型的输出效果

## knowledge_tuning_data_sample.txt 
只有Input 没有答案。Input是问题，不是知识。

可以用于生成新的训练数据

## llama_data.json
提供了instruction 和 output，有八千多组，用于微调
数据构造方法：医学知识库。参考了[cMeKG](https://github.com/king-yyf/CMeKG_tools)。
知识库示例如下:


```

{"中心词": "偏头痛", "相关疾病": ["妊娠合并偏头痛", "恶寒发热"], "相关症状": ["皮肤变硬", "头部及眼后部疼痛并能听到连续不断的隆隆声", "晨起头痛加重"], "所属科室": ["中西医结合科", "内科"], "发病部位": ["头部"]}

```

我们利用GPT3.5接口围绕医学知识库构建问答数据，设置了多种Prompt形式来充分利用知识，于是得到了这个JSON文件
提示词：给定上述知识，生成一个与知识相关的问题，并进行回答

## data-literature\liver_cancer.json
提供了一个数组，每一项都包含instruction 和 output
instruction涉及多个<user>和<bot>，并且最后一句是<user>
用于多轮对话的微调
来源：医学文献
收集了2023年关于肝癌疾病的中文医学文献，利用GPT3.5接口围绕医学文献的【结论】构建多轮问答数据。例子：case.png
目前，我们只开放针对"肝癌"单个疾病训练的模型参数。在未来，我们计划发布融入文献结论的医学对话数据集，并且会针对“肝胆胰”相关16种疾病训练模型。
相关细节：[《探索大模型从医学文献中交互式知识的获取》](https://arxiv.org/pdf/2309.04198.pdf)

### 读论文

**双逻辑能力**：模型面对事实问题 Q 和反事实问题 ¬Q 时保持立场一致

研究发现，现有 LLMs 在私有化后双逻辑能力显著下降，而**融入通用领域双逻辑数据**可同时提升双逻辑能力和回答准确率

从中国知网构建了包含 10,313 篇医学文献摘要、1,212 轮医疗对话、10,908 条测试样本的医疗数据集，以及涵盖 380 对样本的通用双逻辑数据集

数据集包括预训练 （PT） 阶段的医学文献摘要、指令微调 （IFT） 阶段的医学多轮对话和一般领域双逻辑数据，以及医学双逻辑测试数据

构建多轮对话：

- **信息拆解**：将非结构化的医学文献摘要转化为半结构化格式。我们采用正则表达式技术提取关键信息，包括研究目的、方法、实验和结论。当正则表达式技术无法奏效时，可进一步使用 ChatGPT 进行信息提取。
- **提示聚焦**：近期研究表明，大语言模型在处理过长文本时容易分心。为解决这一问题，可通过策略性设计提示词，引导大语言模型将注意力集中在对话中需要纳入的特定信息上。例如，提示词可设计为 “请将 [结论] 纳入对话，其他细节如 [目的]、[方法]、[结果] 等可根据需要参考”。
- **反向验证**：实证观察表明，生成的对话通常会包含医学文献中的原文内容。因此，我们使用最长公共子串匹配算法评估文献信息的纳入程度，以便有选择地保留高质量对话。
- **数据清洗**：生成的对话往往包含大量主观表达，例如 “本研究表明...” 之类的短语。当这些数据用于训练阶段时，大语言模型自然会复制类似的主观表达，在与人类交互时可能导致令人困惑的主观陈述。为解决这一问题，我们使用 ChatGPT 将此类主观表达改写为客观表达，例如将 “This study shows that...” 改为 “Relevant study indicates that...”。

最后构建了 1,212 轮多轮对话，平均每轮对话有 3.53 个回合，第一轮由 ChatGPT 生成自然语言提问，后续轮次强制引用文献摘要



双逻辑测试数据：

AS、ANW 和 SEP 分别表示反义词替换、添加负词和交换实体位置

标注者首先从文献摘要中提炼出事实陈述。通过特定操作将事实陈述转化为反事实陈述，具体包括：

- **反义词替换（AS）**：替换关键词为反义词（如 “较小”→“较大”）；
- **否定词添加（ANW）**：添加否定词改变语义（如 “具备镇痛效果”→“不具备镇痛效果”）；
- **实体位置交换（SEP）**：交换比较对象的位置（如 “MRI 比螺旋 CT 准确率更高”→“螺旋 CT 比 MRI 准确率更高”）。

将事实陈述和反事实陈述分别转化为一般性问题，形成双逻辑测试样本对

最终，我们共标注了**100 对反义词替换样本、58 对否定词添加样本和 44 对实体位置交换样本**

这个方法可以防止训练数据泄露，因为：信息全部来源于摘要，这些是之前模型训练的时候没有接触到的



另外，用 ChatGPT 从通用域自动生成双逻辑数据。最初，我们以两种方式收集事实陈述：从维基百科获取和通过提示 ChatGPT 生成。随后，我们利用 ChatGPT 通过反义词替换、添加否定词和交换实体位置等作生成反事实陈述。按照这一步，我们提示 ChatGPT 将事实陈述和反事实陈述转化为相应的一般问题。最后，我们指示 ChatGPT 根据原始事实陈述制作响应。总体而言，我们为反义词构建了 130 对，150否定，100交换位置

![image-20250609080904989](D:\app\typora\image-20250609080904989.png)

评估指标

1 双逻辑能力：对于一组测试样本，要求两个得到的答案是相反的

2 准确率：基于单个测试样本计算

3 综合上面两个 得到F1分数

模型在私有化过程中，虽然单样本准确率（Accuracy）有所提升，但这是以牺牲双逻辑能力为代价的。

意义：当患者询问某种治疗方案是否有效时，模型如果在面对正向和反向提问时给出不一致的回答，可能会误导患者和医生

发现：对于反义词替换 能轻松从数据学会。但对于比较关系不好学习 所以提升较慢

若基础模型先在通用双逻辑数据上 “预热”，再进行医疗微调，DL-Index 比直接微调高**18.32%**，说明基础模型的逻辑能力可迁移至领域模型。



值得注意的是 这里有两部分 双逻辑数据 其中 通用数据用于预训练 而专用数据用于测试

先在医学文献摘要上进行预训练，再进行指令微调。发现预训练不是决定性因素，加入的通用领域微调数据才是对于双逻辑能力的决定因素




## 知识微调
没有找到对应的json。但论文里提供了微调方案[《基于知识微调的大语言模型可靠中文医学回复生成方法》](https://arxiv.org/abs/2309.04175) 
样例在process_of_knowledge_tuning.png

解读：
知识微调（Knowledge-tuning）是一种让大语言模型在推理时显式利用医学知识库的技术。其核心思想是：模型不仅仅依赖自身参数中的“隐性知识”，而是能够主动检索、调用结构化医学知识库中的“显性知识”，从而生成更为专业、可信的医学回答。
本质上 是练习知识库相关的 function call 能力
意义：指令微调成为将大语言模型快速适配垂直领域的解决方案，然而对于知识正确性要求较高的领域，如医学，指令微调后的模型回复依旧可能存在错误

---

第一阶段：参数填充（Parameter Filling）

- **目标**：从用户提出的问题中，自动提取出知识检索所需的参数。
- **操作**：模型分析问题，识别出“中心实体”（如疾病名称）和“属性”（如治疗方案）。
- **示例**：对于问题“一个5岁的男童最近有一段时间频繁发生耳痛，听力下降等症状，经检查确诊为急性中耳炎。请问医生应该如何治疗？”，模型会提取出：
  - **中心词/实体**：儿童急性中耳炎（Pediatric Acute Otitis Media）
  - **属性**：治疗方案（Treatment Plan）

---

第二阶段：知识函数调用（Knowledge Function Call）

- **目标**：根据第一阶段提取的参数，从医学知识库中检索相关知识。
- **操作**：调用知识检索函数（如`acquire_knowledge(entity, attribute)`），输入实体和属性，返回对应的结构化知识内容。
- **示例**：调用`acquire_knowledge(“儿童急性中耳炎”, “治疗方案”)`和`acquire_knowledge(“儿童急性中耳炎”, “药物治疗”)`，返回如下知识：
  - 治疗方案包括：全身治疗、抗生素治疗、局部治疗、观察
  - 药物治疗包括：阿莫西林、头孢菌素类、大环内酯类、镇痛药

---

第三阶段：基于知识生成回答（Response Generation with Knowledge）

- **目标**：结合问题和检索到的知识，生成专业、可信的医学回答。
- **操作**：模型将原始问题和知识库返回的内容整合，生成包含知识支撑的答案。
- **示例**：模型输出如下内容：
  - “根据当前的治疗标准，初期不需要使用药物，而是以观察为主。但如果症状在48小时后没有改善，医生建议考虑使用抗生素进行治疗，通常建议使用阿莫西林等抗生素类药物。”
  - 并在回答中引用了知识库内容，如“阿莫西林、头孢菌素类、大环内酯类、镇痛药”等。

---

总结

通过上述三步，知识微调让大语言模型能够：
1. 自动理解和结构化用户问题
2. 精准检索医学知识库中的相关知识
3. 生成有据可依、专业可信的医学回答

这种方式极大提升了模型在医学场景下的可靠性和专业性，避免了“幻觉”式的错误回答，使其更适合在医疗等高要求领域应用。

### 读论文
数据来源：CMeKG 知识图谱、丁香园医学指南。为了提高数据质量，已聘请医学专家严格检查和纠正不准确之处
以 7：1：2 的比例分为训练集、验证集和测试集。

如何评估正确性：在一般领域，生成模型评估经常采用诸如 BLEU 和 ROUGE 等指标，以衡量模型输出与真实值之间的相似程度。然而，这些指标可能并不适用于医学问答评估。特别是在生物医学领域，仅依赖输出与真实值的相似性可能无法有效地反映生成答案的质量。图 1 说明了这一局限性，其中一个模型错误地将 “肝胆结石” 的答案建议为 “利福平”。值得注意的是，即使出现如此重大的错误，像 BLEU 这样的指标仍然能给出高分，这凸显了以相似性为中心的评估在生物医学领域的局限性
因此，我们从三个不同的角度严格审视知识微调的效果。(1) 我们引入针对医学实体和知识的数值指标，因为回复的准确性往往反映了它们所涉及的实体和知识。(2) 医学专家对模型输出进行评估，这种评估比自动指标更为细致入微。(3) ChatGPT 作为辅助评估标准。为确保全面评估回复质量，我们提倡应用 H 2（有用性与无害性）分数。“有用性” 反映模型回复中展现的医学专业水平。医学专家需根据所运用的相关医学知识，而非自身医疗技能，对回复的有用性进行评分。这种方法能更准确地体现大语言模型如何利用检索到的知识。同时，“无害性” 旨在识别回复中可能误导用户并使其陷入危险的任何内容，比如错误的用药建议。

评估有用性：
对于检索到医学知识的回答，“3” 级表示相关医学知识的全面覆盖；评级为 “2” 表示响应虽然遗漏了关键信息，但仍然有效；而 “1” 的评级表示完全没有帮助。
对于未包含检索到的知识的回答，它们在 “3” 和 “1” 之间进行评分，分别象征与其中包含的医学知识相关的有效性、充分性和不可接受性。

评估无害性：
“3” 分表示响应中没有有害内容；“2” 表示存在错误但无害的信息；“1” 强调包含有害信息

关键指标：
实体预测准确率：模型从问题中正确识别医疗实体（如疾病、药物）的比例。
知识检索准确率：基于预测的实体和属性，从知识库中正确检索到相关知识的比例。
发现，基于向量嵌入和余弦相似度的密集检索< BM25（统计检索）< LLM

实验结果：知识微调模型评分（2.74）比指令微调模型（2.47）高 11%
发现：当知识检索错误时（约 28.6% 样本），ChatGPT 评分可能被拉低。但仅分析知识检索正确的样本时，评分升至 2.79，接近 ChatGPT 自身水平
结论：响应质量高度依赖知识检索的准确性，若能进一步优化实体 - 属性预测（如减少属性歧义），评分可进一步提升。

讨论：
1 基于知识库的问答对生成，GPT可能会生成错误的内容
例如，例子是 生成了不在知识库的内容 并且其中有错误的内容

2 知识调优的有效性本质上与医疗实体和属性的生成相关，因此测试了 如果样本量很少，能否准确识别属性和实体
发现 100样本的话不行，200样本的话正确率能达到80,800样本的话能达到86.7
传统机器学习模型（如 CRF、SVM）通常需要数千例标注数据才能达到类似性能。因此这个结果已经相对好了
改进方向：
下限阈值：数据量低于 200 例时，模型性能波动较大，需结合数据增强（如实体名称同义词生成、属性关系模拟）或迁移学习（利用相似疾病的数据）提升稳定性。
长尾实体处理：即使使用完整数据集，仍可能存在未覆盖的 “全新实体”（如实验性药物），需结合实时知识检索（如对接 FDA 新药数据库）补充动态知识。

3 对未见过的医疗实体的泛化能力
按实体比例（0.05%~60%）随机抽取不同数量的唯一实体，确保测试集中存在训练集未包含的实体。
示例：若训练集包含 0.05% 实体（约 4 例，如 “肺炎”“糖尿病”），测试集包含其余 99.95% 实体（如 “肝胆管结石”“急性白血病”）。
当使用极其有限的数据进行训练时，Bloom 模型的性能不足。但是，当使用不少于 0.05% 的不同实体进行训练时，它展示了强大的泛化能力。
推论：即使训练集未覆盖测试实体，模型通过学习少量实体的属性关联模式

## 整体PDF
阅读doc文件夹里那个pdf。我希望只记录重要的细节，并通过cursor强化记忆
提出这个方案的原因：大语言模型在垂直领域，如医学领域上的生成内容知识性有限

这是因为：1药物等术语在模型训练阶段曝光率有限
2基于自回归的生成式模型难以掌握疾病、药物等实体间的关系

出于数据隐私考虑，需要本地部署私有化大语言模型

全参微调的缺点：
数据需求大： 微调需要大量标注数据来调整模型
灵活性有限： 一旦模型被微调到特定任务，它在其他任务上的表现可能会受到影响。
但如果用lora，任务切换时只需加载不同的LoRA权重即可。
任务迁移困难： 迁移到另一个任务上需要重新进行微调
LoRA只需为新任务训练一组新的LoRA权重
泛化能力差： 导致模型在特定数据分布上表现出色，但在其他数据分布上的泛化能力较差

p-tuning和prefix-tuning正是PEFT中的两大代表技术
prefix-tuning/p-tuning：只在输入端加“提示”，训练提示参数。
P-Tuning 的tunable soft prompt仅限于输入层，位置可选，不一定是前缀
LoRA：直接在模型的某些层（如注意力层、全连接层）插入“低秩矩阵”，只训练这些插入的参数，主模型参数不变。
lora提出的背景：显存不足严重影响了模型的训练、部署。因此要降低可训练的参数量，降低模型权重和优化器占用的空间
解决方案：用两个低秩矩阵乘积近似完整的矩阵。这里可以读一下论文

添加模板的意义：将下游任务目标转化为预训练任务目标
使用了PET（Pattern-Exploiting Training，模式利用训练）方法
Pattern-Verbalizer-Pair（PVP）：
Pattern（提示模板）：把原始句子转换成带有[MASK]的模板句。例如，“I like the Disney films very much.”可以变成“It was [MASK].”或“I think it is [MASK].”
Verbalizer（标签映射）：把标签（如positive/negative）映射为具体的词（如great/terrible、nice/bad、wonderful/boring），让模型通过预测[MASK]位置的词来判断标签。

把原始句子填入不同的Pattern，模型在[MASK]位置预测词的概率。
通过Verbalizer，把预测的词概率映射到标签空间（如positive/negative）。

对不同Pattern和不同Verbalizer下的预测结果进行加权/投票集成，最终确定标签。
这种集成方式可以提升模型的稳定性和准确性。

这篇工作的意义：十亿级参数量的本草在经过指令微调和与千亿级模型有一定可比性